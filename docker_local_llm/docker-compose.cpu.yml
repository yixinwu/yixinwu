# CPU 模式配置（无 GPU 加速，用于测试或没有 NVIDIA GPU 的情况）
version: '3.8'

services:
  llm-qwen3:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: qwen3-30b-a3b-q4-cpu
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:rw
      - ./entrypoint.sh:/entrypoint.sh:ro
    working_dir: /models
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    command: >
      --model /models/Qwen3-30B-A3B-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      --ctx-size 4096
      --threads 16
    stdin_open: true
    tty: true
    restart: unless-stopped
