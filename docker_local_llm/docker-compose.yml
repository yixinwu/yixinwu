version: '3.8'

services:
  llm-qwen3:
    # 使用 GitHub Container Registry 镜像
    # 如果网络受限，请先执行 ./pull-image.sh 使用代理拉取
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: qwen3-30b-a3b-q4
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:rw
      - ./entrypoint.sh:/entrypoint.sh:ro
      # 挂载 NVIDIA 驱动 (如果不用 nvidia runtime)
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1:ro
    working_dir: /models
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    stdin_open: true
    tty: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
